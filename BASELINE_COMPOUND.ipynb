{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from process import compress_video\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers import Input, TimeDistributed, Conv2D, MaxPooling2D, Flatten, LSTM, Dense\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Load 및 추출 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def quadrant_diff(arr, highlight_map):\n",
    "    \"\"\"\n",
    "    Calculate differences within each quadrant of the frame and update highlight_map based on these differences, \n",
    "    using the standard deviation of all differences as the threshold.\n",
    "\n",
    "    Args:\n",
    "    arr (List of np.array): Each element is a 9x256x256x1 numpy array representing a frame.\n",
    "\n",
    "    Returns:\n",
    "    List: Updated highlight_map indicating highlights based on quadrant differences.\n",
    "    \"\"\"\n",
    "    all_diffs = []\n",
    "    \n",
    "    for i in range(len(arr) - 1):\n",
    "        for quadrant in range(4):\n",
    "            # 4개로 분리된 frame의 차이를 계산하는 부분\n",
    "            quarter_shape = (arr[i].shape[1] // 2, arr[i].shape[2] // 2)\n",
    "            x_start = (quadrant % 2) * quarter_shape[0]\n",
    "            y_start = (quadrant // 2) * quarter_shape[1]\n",
    "            current_quarter = arr[i][:, x_start:x_start + quarter_shape[0], y_start:y_start + quarter_shape[1], :]\n",
    "            next_quarter = arr[i + 1][:, x_start:x_start + quarter_shape[0], y_start:y_start + quarter_shape[1], :]\n",
    "\n",
    "            # 각 frame의 차이를 계산\n",
    "            diff = np.abs(current_quarter - next_quarter).sum()\n",
    "            all_diffs.append(diff)\n",
    "    \n",
    "    # 계산된 frame 차이의 표준편차를 임계값으로 설정\n",
    "    threshold = np.std(all_diffs)\n",
    "    \n",
    "    # 해당 임계값을 바탕으로 frame 라벨 update\n",
    "    for i in range(len(arr) - 1):\n",
    "        count_above_threshold = 0\n",
    "        for quadrant in range(4):\n",
    "            quarter_shape = (arr[i].shape[1] // 2, arr[i].shape[2] // 2)\n",
    "            x_start = (quadrant % 2) * quarter_shape[0]\n",
    "            y_start = (quadrant // 2) * quarter_shape[1]\n",
    "            current_quarter = arr[i][:, x_start:x_start + quarter_shape[0], y_start:y_start + quarter_shape[1], :]\n",
    "            next_quarter = arr[i + 1][:, x_start:x_start + quarter_shape[0], y_start:y_start + quarter_shape[1], :]\n",
    "            \n",
    "            diff = np.abs(current_quarter - next_quarter).sum()\n",
    "\n",
    "            if diff > threshold:\n",
    "                count_above_threshold += 1\n",
    "        \n",
    "        if count_above_threshold == 4:\n",
    "            highlight_map[i] += 1\n",
    "        elif count_above_threshold >= 1:\n",
    "            highlight_map[i] = 2\n",
    "        else:\n",
    "            highlight_map[i] = 0\n",
    "\n",
    "    return highlight_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON data\n",
    "with open('test.json', 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "def parse_annotations(annotation:list, block_num:int):\n",
    "    \"\"\"\n",
    "    Extracts Every Annotation from json label file\n",
    "    \n",
    "    Args:\n",
    "    annotations(List): List of Dictionary for annotations label with highlight and represent\n",
    "\n",
    "    Returns:\n",
    "    List: Whether each block is Highlight or not\n",
    "    \"\"\"\n",
    "    global video_path\n",
    "    highlight_map = {}\n",
    "\n",
    "    video_path = annotation[\"video_path\"]\n",
    "    annotations = annotation[\"annots\"]\n",
    "    \n",
    "    for annot in annotations:\n",
    "        highlights = annot['highlight']\n",
    "        \n",
    "\n",
    "        for num in highlights:\n",
    "            highlight_map[num] = 1\n",
    "            \n",
    "    ret = [0 for _ in range(block_num)]\n",
    "    \n",
    "    for key in highlight_map.keys():\n",
    "        ret[key] = 1\n",
    "\n",
    "    video_frames = np.load(video_path)\n",
    "    ret = quadrant_diff(video_frames, ret)\n",
    "                \n",
    "    return video_frames, ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(\"RuntimeError in set_memory_growth:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, TimeDistributed, Conv2D, MaxPooling2D, Flatten, LSTM, Dense\n",
    "\n",
    "def create_cnn_lstm_model():\n",
    "    # Define the input layer\n",
    "    inputs = Input(shape=(9, 256, 256, 1))\n",
    "\n",
    "    # CNN Layers\n",
    "    x = TimeDistributed(Conv2D(32, (3, 3), activation='relu'))(inputs)\n",
    "    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n",
    "    x = TimeDistributed(Flatten())(x)\n",
    "\n",
    "    # LSTM Layer\n",
    "    x = LSTM(50)(x)\n",
    "\n",
    "    # Output Layer\n",
    "    outputs = Dense(3, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Instantiate and compile the model\n",
    "with tf.device('/GPU:0'):  # 첫 번째 GPU를 사용\n",
    "    model = create_cnn_lstm_model()\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 9, 256, 256, 1)]  0         \n",
      "                                                                 \n",
      " time_distributed_12 (TimeDi  (None, 9, 254, 254, 32)  320       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_13 (TimeDi  (None, 9, 127, 127, 32)  0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_14 (TimeDi  (None, 9, 516128)        0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 50)                103235800 \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 153       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 103,236,273\n",
      "Trainable params: 103,236,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Trainer 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model):\n",
    "    # data_list = os.listdir(\"processed/video\") # 동영상 데이터 \n",
    "    json_path = 'processed\\label\\processed_video_data.json'\n",
    "\n",
    "    with open(json_path, 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    train_length = int(len(json_data) * 0.7)\n",
    "    \n",
    "    train_data = json_data[:train_length]\n",
    "    test_data = json_data[train_length:]\n",
    "    all_histories = []\n",
    "\n",
    "    ## 학습 부분\n",
    "    for i, json_dict in enumerate(train_data):\n",
    "        video, label = parse_annotations(json_dict, json_dict['three_secs'][-1])\n",
    "        \n",
    "        X = video # i번 영상의 npy 파일\n",
    "        y = np.array(label) # 1번 영상에 대한 각 블럭의 하이라이트 여부\n",
    "        y = to_categorical(y, num_classes=3)\n",
    "        \n",
    "        history = model.fit(X, y)\n",
    "        all_histories.append(history.history)  # Save history\n",
    "\n",
    "    ## Test\n",
    "    for i in range(len(test_data)):\n",
    "        label = np.array(parse_annotations(json_data[i]['annots'], json_data[i]['three_secs'][-1] + 1))\n",
    "        \n",
    "        X = test_data[i] # i번 영상의 npy 파일\n",
    "        y = label # 1번 영상에 대한 각 블럭의 하이라이트 여부\n",
    "        \n",
    "        print(f\"Test {i} :: {model.evaluate(X, y)}\")\n",
    "\n",
    "    model.save('./video_model.h5')\n",
    "\n",
    "    return model, all_histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(histories):\n",
    "    epochs = range(1, len(histories[0]['loss']) + 1)\n",
    "    all_loss = [h['loss'] for h in histories]\n",
    "    all_acc = [h['accuracy'] for h in histories]\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for i, loss in enumerate(all_loss):\n",
    "        plt.plot(epochs, loss, label=f'Training {i+1}')\n",
    "    plt.title('Loss over training videos')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for i, acc in enumerate(all_acc):\n",
    "        plt.plot(epochs, acc, label=f'Training {i+1}')\n",
    "    plt.title('Accuracy over training videos')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_traied, history = trainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
